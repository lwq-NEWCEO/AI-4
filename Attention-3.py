import os
import warnings

warnings.filterwarnings('ignore')  # å¿½ç•¥æ— å…³è­¦å‘Š
# è§£å†³OpenMPå†²çª+GPUå†…å­˜ä¼˜åŒ–
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["TORCH_CUDNN_V8_API_DISABLED"] = "1"

import matplotlib.pyplot as plt
import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, random_split, Subset
from torchvision import datasets, transforms, models
from torchvision.models.resnet import BasicBlock, ResNet
from tqdm import tqdm  # è¿›åº¦æ¡å¯è§†åŒ–
import copy  # ä¿å­˜æœ€ä½³æ¨¡å‹


# --- 1. æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼šéšæœºç§å­+æ—©åœæœºåˆ¶ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ ---
def set_seed(seed=42):
    """å›ºå®šå…¨é“¾è·¯éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False  # å…³é—­CuDNNè‡ªåŠ¨ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥ä¿è¯ç¨³å®šæ€§


class EarlyStopping:
    """æ—©åœæœºåˆ¶ï¼šéªŒè¯æŸå¤±è¿ç»­ä¸ä¸‹é™åˆ™åœæ­¢è®­ç»ƒï¼Œä¿å­˜æœ€ä½³æ¨¡å‹"""

    def __init__(self, patience=10, min_delta=1e-4, path='best_model.pth'):
        self.patience = patience  # å®¹å¿å¤šå°‘ä¸ªepochæ— æå‡
        self.min_delta = min_delta  # æœ€å°æå‡é˜ˆå€¼
        self.path = path  # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
        self.counter = 0  # æ— æå‡è®¡æ•°å™¨
        self.best_loss = float('inf')  # æœ€ä½³éªŒè¯æŸå¤±
        self.early_stop = False  # æ˜¯å¦æ—©åœæ ‡å¿—

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            # éªŒè¯æŸå¤±ä¸‹é™ï¼Œæ›´æ–°æœ€ä½³æŸå¤±å¹¶ä¿å­˜æ¨¡å‹
            self.best_loss = val_loss
            self.counter = 0
            torch.save(model.state_dict(), self.path)
            print(f"âœ… éªŒè¯æŸå¤±ä¸‹é™è‡³ {val_loss:.4f}ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            # éªŒè¯æŸå¤±æ— æå‡ï¼Œè®¡æ•°å™¨+1
            self.counter += 1
            print(f"âš ï¸  æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"ğŸš« è§¦å‘æ—©åœï¼æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")


# --- 2. æ³¨æ„åŠ›æœºåˆ¶ï¼šSE Blockï¼ˆè½»é‡é«˜æ•ˆï¼Œé€‚é…ResNetï¼‰ ---
class SEBlock(nn.Module):
    """Squeeze-and-Excitation Blockï¼šé€šé“æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        # Squeezeï¼šå…¨å±€å¹³å‡æ± åŒ–ï¼ˆå‹ç¼©ç©ºé—´ç»´åº¦ï¼‰
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # Excitationï¼šå…¨è¿æ¥å±‚ï¼ˆå­¦ä¹ é€šé“æƒé‡ï¼‰
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),  # é™ç»´
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),  # å‡ç»´
            nn.Sigmoid()  # è¾“å‡º0-1çš„æƒé‡
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        # å…¨å±€å¹³å‡æ± åŒ–ï¼š(b,c,h,w) â†’ (b,c,1,1) â†’ (b,c)
        y = self.avg_pool(x).view(b, c)
        # å­¦ä¹ é€šé“æƒé‡ï¼š(b,c) â†’ (b,c)
        y = self.fc(y).view(b, c, 1, 1)
        # ç‰¹å¾åŠ æƒï¼šé€é€šé“ç›¸ä¹˜
        return x * y.expand_as(x)


# --- 3. æ”¹è¿›ResNet18ï¼šé›†æˆSE Blockï¼ˆæ›¿æ¢åŸæœ‰BasicBlockï¼‰ ---
class SEBasicBlock(BasicBlock):
    """å¸¦SEæ³¨æ„åŠ›çš„ResNetåŸºç¡€å—"""

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, reduction=16):
        super(SEBasicBlock, self).__init__(
            inplanes, planes, stride, downsample, groups, base_width, dilation, norm_layer
        )
        # åœ¨Blockæœ«å°¾æ·»åŠ SE Block
        self.se = SEBlock(planes, reduction)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        # æ’å…¥SEæ³¨æ„åŠ›åŠ æƒ
        out = self.se(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity  # æ®‹å·®è¿æ¥
        out = self.relu(out)

        return out


# æ„å»ºSE-ResNet18
def se_resnet18(pretrained=False, num_classes=1000, reduction=16):
    """å¸¦SEæ³¨æ„åŠ›çš„ResNet18æ¨¡å‹"""
    norm_layer = nn.BatchNorm2d
    model = ResNet(
        SEBasicBlock,  # ç”¨SEBasicBlockæ›¿æ¢é»˜è®¤BasicBlock
        [2, 2, 2, 2],  # ResNet18çš„Blockæ•°é‡
        num_classes=num_classes,
        norm_layer=norm_layer
    )
    model.inplanes = 64
    model.dilation = 1
    model.base_width = 64
    model.groups = 1

    # åˆå§‹åŒ–æƒé‡
    for m in model.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)

    # å¦‚æœéœ€è¦é¢„è®­ç»ƒæƒé‡ï¼ˆè¿™é‡Œæˆ‘ä»¬åç»­åŠ è½½å®˜æ–¹ResNet18é¢„è®­ç»ƒæƒé‡ï¼Œå†é€‚é…SEå±‚ï¼‰
    if pretrained:
        # åŠ è½½å®˜æ–¹ResNet18é¢„è®­ç»ƒæƒé‡
        resnet18_pretrained = models.resnet18(pretrained=True)
        # å¤åˆ¶é™¤æœ€åå…¨è¿æ¥å±‚å¤–çš„æƒé‡ï¼ˆSEå±‚æƒé‡ä¼šéšæœºåˆå§‹åŒ–ï¼‰
        pretrained_dict = resnet18_pretrained.state_dict()
        model_dict = model.state_dict()
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and not k.startswith('fc')}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    return model


# --- 4. è¿ç§»å­¦ä¹ æ¨¡å‹å°è£…ï¼ˆSE-ResNet18ï¼‰ ---
class SETransferModel(nn.Module):
    def __init__(self, num_classes=10, pretrained=True):
        super(SETransferModel, self).__init__()
        # åŠ è½½å¸¦SEæ³¨æ„åŠ›çš„ResNet18ï¼ˆé¢„è®­ç»ƒæƒé‡é€‚é…ï¼‰
        self.base_model = se_resnet18(pretrained=pretrained, num_classes=num_classes)
        # æ›¿æ¢æœ€åå…¨è¿æ¥å±‚ï¼ˆç¡®ä¿è¾“å‡ºé€‚é…CIFAR-10çš„10ç±»ï¼‰
        num_ftrs = self.base_model.fc.in_features
        self.base_model.fc = nn.Sequential(
            nn.Dropout(0.3),  # æ·»åŠ DropoutæŠ‘åˆ¶è¿‡æ‹Ÿåˆ
            nn.Linear(num_ftrs, num_classes)
        )

    def forward(self, x):
        return self.base_model(x)

    def freeze_layers(self, freeze=True):
        """å†»ç»“/è§£å†»å·ç§¯å±‚ï¼ˆä»…è®­ç»ƒ/å¾®è°ƒåˆ†ç±»å¤´ï¼‰"""
        for param in self.base_model.parameters():
            param.requires_grad = not freeze
        # ç¡®ä¿åˆ†ç±»å¤´å§‹ç»ˆå¯è®­ç»ƒ
        for param in self.base_model.fc.parameters():
            param.requires_grad = True


# --- 5. ä¼˜åŒ–åçš„è®­ç»ƒå‡½æ•°ï¼ˆåˆ†é˜¶æ®µå¾®è°ƒ+AdamW+æ—©åœï¼‰ ---
def train_stage_model(model, train_loader, val_loader, epochs, optimizer, scheduler, early_stopping, stage_name):
    """åˆ†é˜¶æ®µè®­ç»ƒå‡½æ•°ï¼ˆé€‚é…ä¸¤é˜¶æ®µå¾®è°ƒï¼‰"""
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘ï¼Œè¿›ä¸€æ­¥æŠ‘åˆ¶è¿‡æ‹Ÿåˆ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}
    best_val_acc = 0.0

    print(f"\n===== å¼€å§‹ {stage_name}ï¼ˆ{epochs}ä¸ªEpochï¼‰=====")
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        # è¿›åº¦æ¡å¯è§†åŒ–
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} (Train)")
        for inputs, labels in train_bar:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡è®­ç»ƒæŒ‡æ ‡
            train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            train_bar.set_postfix({'loss': f"{loss.item():.4f}"})
        train_bar.close()

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            val_bar = tqdm(val_loader, desc=f"Epoch {epoch + 1}/{epochs} (Val)")
            for inputs, labels in val_bar:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                # ç»Ÿè®¡éªŒè¯æŒ‡æ ‡
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                val_bar.set_postfix({'loss': f"{loss.item():.4f}"})
            val_bar.close()

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_train_loss = train_loss / train_total
        avg_train_acc = 100 * train_correct / train_total
        avg_val_loss = val_loss / val_total
        avg_val_acc = 100 * val_correct / val_total
        current_lr = optimizer.param_groups[0]['lr']

        # æ›´æ–°å†å²è®°å½•
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)
        history['lr'].append(current_lr)

        # æ›´æ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc

        # æ‰“å°æ—¥å¿—
        print(f"Epoch {epoch + 1}/{epochs} | LR: {current_lr:.6f} | "
              f"Train Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.2f}% | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.2f}% | "
              f"Best Val Acc: {best_val_acc:.2f}%")

        # æ—©åœæ£€æŸ¥
        early_stopping(avg_val_loss, model)
        if early_stopping.early_stop:
            break

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

    print(f"===== {stage_name}ç»“æŸ =====")
    return history


# --- 6. æµ‹è¯•é›†è¯„ä¼°å‡½æ•°ï¼ˆå«ç±»åˆ«çº§å‡†ç¡®ç‡ï¼‰ ---
def evaluate_test_set(model, test_loader, classes, device):
    """è¯¦ç»†è¯„ä¼°æµ‹è¯•é›†ï¼šæ•´ä½“å‡†ç¡®ç‡+æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡"""
    model.eval()
    test_correct = 0
    test_total = 0
    class_correct = list(0. for _ in range(len(classes)))
    class_total = list(0. for _ in range(len(classes)))

    with torch.no_grad():
        test_bar = tqdm(test_loader, desc="Testing")
        for inputs, labels in test_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            # æ•´ä½“ç»Ÿè®¡
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()

            # ç±»åˆ«çº§ç»Ÿè®¡
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                label = labels[i]
                class_correct[label] += c[i].item()
                class_total[label] += 1

    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    overall_acc = 100 * test_correct / test_total
    print(f"\nğŸ“Š æµ‹è¯•é›†æ•´ä½“å‡†ç¡®ç‡: {overall_acc:.2f}%")

    # è®¡ç®—ç±»åˆ«çº§å‡†ç¡®ç‡
    print("\nç±»åˆ«çº§å‡†ç¡®ç‡ï¼š")
    for i in range(len(classes)):
        if class_total[i] > 0:
            class_acc = 100 * class_correct[i] / class_total[i]
            print(f"  {classes[i]:<10}: {class_acc:.2f}%")
        else:
            print(f"  {classes[i]:<10}: æ— æ•°æ®")

    return overall_acc, class_correct, class_total


# --- 7. å¯è§†åŒ–å‡½æ•°ï¼ˆå¯¹æ¯”è®­ç»ƒæ›²çº¿ï¼‰ ---
def plot_combined_history(history_stage1, history_stage2, save_path='se_resnet18_training_history.png'):
    """åˆå¹¶ä¸¤é˜¶æ®µè®­ç»ƒå†å²ï¼Œç»˜åˆ¶ç»¼åˆæ›²çº¿"""
    # åˆå¹¶ä¸¤é˜¶æ®µæ•°æ®
    total_train_loss = history_stage1['train_loss'] + history_stage2['train_loss']
    total_train_acc = history_stage1['train_acc'] + history_stage2['train_acc']
    total_val_loss = history_stage1['val_loss'] + history_stage2['val_loss']
    total_val_acc = history_stage1['val_acc'] + history_stage2['val_acc']
    total_lr = history_stage1['lr'] + history_stage2['lr']
    total_epochs = len(total_train_loss)

    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(18, 5))

    # 1. æŸå¤±æ›²çº¿
    plt.subplot(1, 3, 1)
    plt.plot(total_train_loss, label='Train Loss', color='#1f77b4', linewidth=1.5)
    plt.plot(total_val_loss, label='Val Loss', color='#ff7f0e', linewidth=1.5)
    plt.axvline(x=len(history_stage1['train_loss']) - 1, color='red', linestyle='--', alpha=0.7, label='Stage 1 End')
    plt.title('Loss History (SE-ResNet18)', fontsize=12)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(alpha=0.3)

    # 2. å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 3, 2)
    plt.plot(total_train_acc, label='Train Accuracy', color='#1f77b4', linewidth=1.5)
    plt.plot(total_val_acc, label='Val Accuracy', color='#ff7f0e', linewidth=1.5)
    plt.axvline(x=len(history_stage1['train_acc']) - 1, color='red', linestyle='--', alpha=0.7, label='Stage 1 End')
    plt.title('Accuracy History (SE-ResNet18)', fontsize=12)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(alpha=0.3)

    # 3. å­¦ä¹ ç‡æ›²çº¿
    plt.subplot(1, 3, 3)
    plt.plot(total_lr, label='Learning Rate', color='#2ca02c', linewidth=1.5)
    plt.axvline(x=len(history_stage1['lr']) - 1, color='red', linestyle='--', alpha=0.7, label='Stage 1 End')
    plt.title('Learning Rate Schedule', fontsize=12)
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.legend()
    plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"\nè®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    # 1. åŸºç¡€é…ç½®ï¼ˆæœ€ä¼˜å‚æ•°é€‚é…ï¼‰
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. æ•°æ®é¢„å¤„ç†ï¼ˆå¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼šæ›´é€‚é…ResNetï¼‰
    # è®­ç»ƒé›†å¢å¼ºï¼ˆä¿ç•™æœ‰æ•ˆç­–ç•¥ï¼Œæ–°å¢éšæœºæ—‹è½¬ï¼‰
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),  # éšæœºè£å‰ª+å¡«å……
        transforms.RandomHorizontalFlip(p=0.5),  # æ°´å¹³ç¿»è½¬
        transforms.RandomRotation(degrees=15),  # éšæœºæ—‹è½¬Â±15Â°
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # æ¸©å’Œé¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.2)  # éšæœºæ“¦é™¤ï¼ˆæ¨¡æ‹Ÿé®æŒ¡ï¼Œæå‡é²æ£’æ€§ï¼‰
    ])

    # éªŒè¯é›†/æµ‹è¯•é›†ï¼šä»…æ ‡å‡†åŒ–ï¼ˆæ— å¢å¼ºï¼‰
    transform_test_val = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))
    ])

    # 3. æ•°æ®é›†åŠ è½½ä¸åˆ’åˆ†ï¼ˆ45kè®­ç»ƒ+5kéªŒè¯ï¼‰
    full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=False, transform=None)
    train_indices, val_indices = random_split(range(len(full_train_dataset)), [45000, 5000])

    train_dataset = Subset(
        datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train),
        train_indices.indices
    )
    val_dataset = Subset(
        datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_test_val),
        val_indices.indices
    )
    test_dataset = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test_val)

    # 4. DataLoaderé…ç½®ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´Batch Sizeï¼‰
    BATCH_SIZE = 256  # æœ€ä¼˜Batch Sizeï¼ˆGPUå†…å­˜â‰¥8Gæ¨èï¼Œå¦åˆ™æ”¹ä¸º128/64ï¼‰
    NUM_WORKERS = 4 if device.type == 'cuda' else 0  # GPUç”¨4è¿›ç¨‹ï¼ŒCPUç”¨0è¿›ç¨‹

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,
        pin_memory=True, drop_last=True  # pin_memoryåŠ é€ŸGPUæ•°æ®ä¼ è¾“ï¼Œdrop_lasté¿å…æ‰¹æ¬¡ä¸ä¸€è‡´
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=NUM_WORKERS,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=NUM_WORKERS,
        pin_memory=True
    )

    # 5. æ¨¡å‹åˆå§‹åŒ–
    model = SETransferModel(num_classes=len(classes), pretrained=True).to(device)
    print(f"\nğŸ“¦ æ¨¡å‹ç»“æ„ï¼šSE-ResNet18ï¼ˆå¸¦é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼‰")
    print(f"ğŸ“¦ æ¨¡å‹å‚æ•°æ€»æ•°ï¼š{sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ“¦ å¯è®­ç»ƒå‚æ•°æ€»æ•°ï¼š{sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # 6. ä¸¤é˜¶æ®µè®­ç»ƒé…ç½®ï¼ˆæœ€ä¼˜è¿ç§»å­¦ä¹ ç­–ç•¥ï¼‰
    # é˜¶æ®µ1ï¼šå†»ç»“å·ç§¯å±‚ï¼Œä»…è®­ç»ƒåˆ†ç±»å¤´ï¼ˆ5ä¸ªEpochï¼Œå¿«é€Ÿæ”¶æ•›åˆ†ç±»å¤´ï¼‰
    model.freeze_layers(freeze=True)
    optimizer_stage1 = optim.AdamW(
        model.parameters(), lr=1e-3, weight_decay=5e-4  # AdamW+L2æ­£åˆ™
    )
    scheduler_stage1 = CosineAnnealingLR(optimizer_stage1, T_max=5, eta_min=1e-5)
    early_stopping = EarlyStopping(patience=5, min_delta=1e-4, path='se_resnet18_best.pth')

    # é˜¶æ®µ1è®­ç»ƒ
    history_stage1 = train_stage_model(
        model, train_loader, val_loader, epochs=5,
        optimizer=optimizer_stage1, scheduler=scheduler_stage1,
        early_stopping=early_stopping, stage_name="é˜¶æ®µ1ï¼šå†»ç»“å·ç§¯å±‚è®­ç»ƒåˆ†ç±»å¤´"
    )

    # é˜¶æ®µ2ï¼šè§£å†»æ‰€æœ‰å±‚ï¼Œå¾®è°ƒæ•´ä¸ªæ¨¡å‹ï¼ˆ95ä¸ªEpochï¼Œé€‚é…CIFAR-10ï¼‰
    model.freeze_layers(freeze=False)  # è§£å†»æ‰€æœ‰å±‚
    optimizer_stage2 = optim.AdamW(
        model.parameters(), lr=1e-4, weight_decay=5e-4  # æ›´å°çš„å­¦ä¹ ç‡ï¼Œé¿å…ç ´åé¢„è®­ç»ƒç‰¹å¾
    )
    scheduler_stage2 = CosineAnnealingLR(optimizer_stage2, T_max=95, eta_min=1e-6)

    # é˜¶æ®µ2è®­ç»ƒï¼ˆé‡ç½®æ—©åœè®¡æ•°å™¨ï¼‰
    early_stopping = EarlyStopping(patience=10, min_delta=1e-4, path='se_resnet18_best.pth')
    history_stage2 = train_stage_model(
        model, train_loader, val_loader, epochs=95,
        optimizer=optimizer_stage2, scheduler=scheduler_stage2,
        early_stopping=early_stopping, stage_name="é˜¶æ®µ2ï¼šè§£å†»æ‰€æœ‰å±‚å¾®è°ƒ"
    )

    # 7. åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°æµ‹è¯•é›†
    print(f"\nğŸ” åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•é›†è¯„ä¼°...")
    model.load_state_dict(torch.load('se_resnet18_best.pth'))
    model.to(device)
    test_acc, class_correct, class_total = evaluate_test_set(model, test_loader, classes, device)

    # 8. å¯è§†åŒ–è®­ç»ƒæ›²çº¿
    plot_combined_history(history_stage1, history_stage2)

    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆå«ç»“æ„+æƒé‡ï¼‰
    torch.save({
        'model_state_dict': model.state_dict(),
        'test_acc': test_acc,
        'classes': classes
    }, 'se_resnet18_final.pth')
    print(f"\nğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: se_resnet18_final.pth")