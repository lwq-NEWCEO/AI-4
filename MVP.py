import os
import warnings
import random
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import copy
import time  # å¯¼å…¥ time æ¨¡å—ç”¨äºè®¡æ—¶

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, random_split, Subset
from torchvision import datasets, transforms, models

# å¯¼å…¥æ–°ç‰ˆæƒé‡æšä¸¾ï¼ˆå¦‚æœä¸å…¼å®¹æ—§ç‰ˆtorchvisionï¼Œä»£ç ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
try:
    from torchvision.models import efficientnet_b2, EfficientNet_B2_Weights
except ImportError:
    from torchvision.models import efficientnet_b2

    EfficientNet_B2_Weights = None

# --- é…ç½®ç¯å¢ƒ ---
warnings.filterwarnings('ignore')
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# --- 1. æ ¸å¿ƒå·¥å…·å‡½æ•° ---
def set_seed(seed=42):
    """å›ºå®šå…¨é“¾è·¯éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=15, min_delta=1e-4, path='best_model.pth'):
        self.patience = patience
        self.min_delta = min_delta
        self.path = path
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            torch.save(model.state_dict(), self.path)
        else:
            self.counter += 1
            if self.counter % 5 == 0:
                print(f"âš ï¸  æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"ğŸš« è§¦å‘æ—©åœï¼æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")


# --- 2. æ¨¡å‹å®šä¹‰ (EfficientNet-B2) ---

class EfficientNetTransfer(nn.Module):
    def __init__(self, num_classes=10, load_pretrained=True):  # å¢åŠ  load_pretrained å‚æ•°
        super(EfficientNetTransfer, self).__init__()

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½ EfficientNet-B2 {'é¢„è®­ç»ƒ' if load_pretrained else 'éšæœºåˆå§‹åŒ–'} æƒé‡...")

        if load_pretrained:
            if EfficientNet_B2_Weights is not None:
                weights = EfficientNet_B2_Weights.DEFAULT
                self.base_model = efficientnet_b2(weights=weights)
            else:
                self.base_model = efficientnet_b2(pretrained=True)
        else:  # ä¸åŠ è½½é¢„è®­ç»ƒï¼Œéšæœºåˆå§‹åŒ–
            self.base_model = efficientnet_b2(pretrained=False)  # EfficientNet é»˜è®¤æ˜¯éšæœºåˆå§‹åŒ–

        original_fc = self.base_model.classifier[1]
        in_features = original_fc.in_features

        self.base_model.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(in_features, num_classes)
        )

    def forward(self, x):
        return self.base_model(x)

    def freeze_layers(self, freeze=True):
        for param in self.base_model.parameters():
            param.requires_grad = not freeze

        for param in self.base_model.classifier.parameters():
            param.requires_grad = True


# --- 3. è®­ç»ƒæµç¨‹å‡½æ•° (ä¸å˜) ---
def train_stage_model(model, train_loader, val_loader, epochs, optimizer, scheduler, early_stopping, stage_name):
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}
    best_val_acc = 0.0

    print(f"\n===== å¼€å§‹ {stage_name}ï¼ˆå…± {epochs} Epochsï¼‰=====")

    for epoch in range(epochs):
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0

        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} [Train]", leave=False)
        for inputs, labels in train_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        avg_train_loss = train_loss / train_total
        avg_train_acc = 100 * train_correct / train_total
        avg_val_loss = val_loss / val_total
        avg_val_acc = 100 * val_correct / val_total
        current_lr = optimizer.param_groups[0]['lr']

        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc

        print(f"Ep {epoch + 1}/{epochs} | LR: {current_lr:.2e} | "
              f"Tr Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.2f}% | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.2f}% (Best: {best_val_acc:.2f}%)")

        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)
        history['lr'].append(current_lr)

        early_stopping(avg_val_loss, model)
        if early_stopping.early_stop:
            break

        scheduler.step()

    return history


# --- 4. è¯„ä¼°æµç¨‹å‡½æ•° (æ·»åŠ æ˜¾å­˜å’Œæ¨ç†æ—¶é—´æµ‹é‡) ---
def evaluate_test_set(model, test_loader, classes, device):
    model.eval()
    test_correct = 0
    test_total = 0
    class_correct = list(0. for _ in range(len(classes)))
    class_total = list(0. for _ in range(len(classes)))

    # æ˜¾å­˜ä½¿ç”¨é‡
    if device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats(device)
        start_mem = torch.cuda.memory_allocated(device)

    start_time = time.time()

    print("\nğŸ” æ­£åœ¨è¯„ä¼°æµ‹è¯•é›†...")
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Testing"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                label = labels[i]
                class_correct[label] += c[i].item()
                class_total[label] += 1

    end_time = time.time()
    inference_time = (end_time - start_time) / test_total * 1000  # å•å¼ å›¾ç‰‡æ¨ç†æ—¶é—´ (ms)
    fps = test_total / (end_time - start_time)  # FPS

    acc = 100 * test_correct / test_total
    print(f"\nğŸ“Š æµ‹è¯•é›†æœ€ç»ˆå‡†ç¡®ç‡: {acc:.2f}%")
    print("-" * 30)
    for i in range(len(classes)):
        if class_total[i] > 0:
            print(f"  {classes[i]:<10}: {100 * class_correct[i] / class_total[i]:.2f}%")
    print("-" * 30)
    print(f"â±ï¸ å¹³å‡å•å¼ å›¾ç‰‡æ¨ç†æ—¶é—´: {inference_time:.2f} ms")
    print(f"âš¡ æ¨ç†é€Ÿåº¦: {fps:.2f} FPS")

    if device.type == 'cuda':
        end_mem = torch.cuda.memory_allocated(device)
        peak_mem = torch.cuda.max_memory_allocated(device)
        print(f"ğŸ“ˆ GPUæ˜¾å­˜å ç”¨ (MB): {peak_mem / (1024 ** 2):.2f} (å³°å€¼)")
        return acc, peak_mem / (1024 ** 2), inference_time

    return acc, None, inference_time


# --- 5. ç»˜å›¾å‡½æ•° (ä¸å˜) ---
def plot_history(hist1, hist2, filename='effnet_b2_history.png', title_suffix=""):
    loss = hist1['train_loss'] + hist2['train_loss']
    val_loss = hist1['val_loss'] + hist2['val_loss']
    acc = hist1['train_acc'] + hist2['train_acc']
    val_acc = hist1['val_acc'] + hist2['val_acc']
    lr = hist1['lr'] + hist2['lr']

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(loss, label='Train Loss')
    plt.plot(val_loss, label='Val Loss')
    plt.axvline(x=len(hist1['train_loss']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title(f'Loss {title_suffix}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 2)
    plt.plot(acc, label='Train Acc')
    plt.plot(val_acc, label='Val Acc')
    plt.axvline(x=len(hist1['train_acc']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title(f'Accuracy {title_suffix}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 3)
    plt.plot(lr, label='LR', color='green')
    plt.axvline(x=len(hist1['lr']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title(f'Learning Rate {title_suffix}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    print(f"\nğŸ“ˆ æ›²çº¿å›¾å·²ä¿å­˜è‡³ {filename}")


# --- ä¸»ç¨‹åº ---
def run_experiment(model_type, pretrained, resize_factor, label_smoothing, random_erasing_p, base_path="."):
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(
        f"\n--- å®éªŒé…ç½®: {model_type} | Pretrained: {pretrained} | Resize: {resize_factor}x | LS: {label_smoothing} | RE_p: {random_erasing_p} ---")
    print(f"ğŸš€ Device: {device}")

    # 1. æ•°æ®å‡†å¤‡
    RESIZE_SIZE = 32 * resize_factor  # åŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡

    transform_train = transforms.Compose([
        transforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),
        transforms.RandomCrop(RESIZE_SIZE, padding=8 if RESIZE_SIZE > 32 else 4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=random_erasing_p)  # åŠ¨æ€è°ƒæ•´ RandomErasing
    ])

    transform_test = transforms.Compose([
        transforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    full_data = datasets.CIFAR10(root=os.path.join(base_path, 'data'), train=True,
                                 download=True)  # download=True ç¡®ä¿æ•°æ®å­˜åœ¨
    train_idx, val_idx = random_split(range(len(full_data)), [45000, 5000])

    train_ds = Subset(datasets.CIFAR10(os.path.join(base_path, 'data'), train=True, transform=transform_train),
                      train_idx.indices)
    val_ds = Subset(datasets.CIFAR10(os.path.join(base_path, 'data'), train=True, transform=transform_test),
                    val_idx.indices)
    test_ds = datasets.CIFAR10(os.path.join(base_path, 'data'), train=False, transform=transform_test)

    BATCH_SIZE = 128
    workers = 4 if device.type == 'cuda' else 0

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=workers, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. åˆå§‹åŒ– EfficientNet-B2
    print(f"\nğŸ”¨ åˆå§‹åŒ– {model_type} ...")
    model = EfficientNetTransfer(num_classes=10, load_pretrained=pretrained).to(device)

    # 3. é˜¶æ®µ 1: å†»ç»“è®­ç»ƒ (Warmup)
    print("\n>>> é˜¶æ®µ 1: å†»ç»“ç‰¹å¾å±‚ (Warmup Classifier)")
    model.freeze_layers(freeze=True)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    # CosineAnnealingLR éœ€è¦ä¼ å…¥å½“å‰çš„ epochsï¼Œè¿™é‡Œ Stage 1 æ˜¯ 5 epochs
    scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)
    stopper = EarlyStopping(patience=5, path=os.path.join(base_path, f'{model_type}_best_s1.pth'))

    hist1 = train_stage_model(model, train_loader, val_loader, 5, optimizer, scheduler, stopper, "Stage 1")

    # 4. é˜¶æ®µ 2: å…¨é¢å¾®è°ƒ
    print("\n>>> é˜¶æ®µ 2: è§£å†»å…¨ç½‘å¾®è°ƒ (Fine-tuning)")
    # åªæœ‰å½“ Stage 1 æˆåŠŸè¿è¡Œå¹¶ä¿å­˜äº†æ¨¡å‹ï¼Œæ‰åŠ è½½
    if not stopper.early_stop:
        model.load_state_dict(torch.load(os.path.join(base_path, f'{model_type}_best_s1.pth')))
    else:  # å¦‚æœ Stage 1 å°±æ—©åœäº†ï¼Œç›´æ¥ç”¨å½“å‰æ¨¡å‹çŠ¶æ€ç»§ç»­
        print("Stage 1 æ—©åœï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€è¿›å…¥ Stage 2")

    model.freeze_layers(freeze=False)

    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)
    # CosineAnnealingLR éœ€è¦ä¼ å…¥å½“å‰çš„ epochsï¼Œè¿™é‡Œ Stage 2 æ˜¯ 150 epochs
    scheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-7)
    stopper = EarlyStopping(patience=20, path=os.path.join(base_path, f'{model_type}_best_s2.pth'))

    hist2 = train_stage_model(model, train_loader, val_loader, 150, optimizer, scheduler, stopper, "Stage 2")

    # 5. ç»“æœ
    plot_history(hist1, hist2, filename=os.path.join(base_path, f'{model_type}_history.png'),
                 title_suffix=f"({model_type})")

    print("\nğŸ† åŠ è½½æœ€ç»ˆæœ€ä½³æ¨¡å‹...")
    # åªæœ‰å½“ Stage 2 æˆåŠŸè¿è¡Œå¹¶ä¿å­˜äº†æ¨¡å‹ï¼Œæ‰åŠ è½½
    if not stopper.early_stop:
        model.load_state_dict(torch.load(os.path.join(base_path, f'{model_type}_best_s2.pth')))
    else:  # å¦‚æœ Stage 2 ä¹Ÿæ—©åœäº†ï¼Œè¯´æ˜å½“å‰æ¨¡å‹å°±æ˜¯æœ€ä½³
        print("Stage 2 æ—©åœï¼Œä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€è¿›è¡Œè¯„ä¼°")

    test_acc, peak_mem, inference_time = evaluate_test_set(model, test_loader, classes, device)

    torch.save(model.state_dict(), os.path.join(base_path, f'final_{model_type}.pth'))
    print(f"\nâœ… å®Œæˆï¼{model_type} æ¨¡å‹å·²ä¿å­˜ã€‚")

    return test_acc, peak_mem, inference_time, model  # è¿”å›æ¨¡å‹å¯¹è±¡ä»¥è®¡ç®—å‚æ•°é‡


if __name__ == '__main__':
    # ç¡®ä¿æœ‰åœ°æ–¹å­˜æ”¾ç»“æœ
    results_dir = "./experiment_results"
    os.makedirs(results_dir, exist_ok=True)

    experiment_results = {}

    # --- å®éªŒ 1: EfficientNet-B2 (Pretrained) ---
    print("\n\n=== è¿è¡Œå®éªŒ: EfficientNet-B2 (Pretrained) (åŸºå‡†æ¨¡å‹) ===")
    acc_pretrained, mem_pretrained, time_pretrained, model_pretrained = run_experiment(
        "EfficientNetB2_Pretrained", pretrained=True, resize_factor=2, label_smoothing=0.1,
        random_erasing_p=0.2, base_path=results_dir
    )
    param_count_pretrained = sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad)
    experiment_results["EfficientNetB2_Pretrained"] = {
        "Accuracy": acc_pretrained, "Memory(MB)": mem_pretrained, "Inference Time(ms)": time_pretrained,
        "Params": param_count_pretrained
    }

    # --- å®éªŒ 2: EfficientNet-B2 (No Pretrain) ---
    print("\n\n=== è¿è¡Œå®éªŒ: EfficientNet-B2 (No Pretrain) ===")
    acc_no_pretrain, mem_no_pretrain, time_no_pretrain, model_no_pretrain = run_experiment(
        "EfficientNetB2_NoPretrain", pretrained=False, resize_factor=2, label_smoothing=0.1,
        random_erasing_p=0.2, base_path=results_dir
    )
    param_count_no_pretrain = sum(p.numel() for p in model_no_pretrain.parameters() if p.requires_grad)
    experiment_results["EfficientNetB2_NoPretrain"] = {
        "Accuracy": acc_no_pretrain, "Memory(MB)": mem_no_pretrain, "Inference Time(ms)": time_no_pretrain,
        "Params": param_count_no_pretrain
    }

    # --- å®éªŒ 3: EfficientNet-B2 (Pretrained) - No Resize (32x32) ---
    print("\n\n=== è¿è¡Œå®éªŒ: EfficientNet-B2 (Pretrained) - No Resize (32x32) ===")
    acc_no_resize, mem_no_resize, time_no_resize, model_no_resize = run_experiment(
        "EfficientNetB2_NoResize", pretrained=True, resize_factor=1, label_smoothing=0.1,
        random_erasing_p=0.2, base_path=results_dir
    )
    param_count_no_resize = sum(p.numel() for p in model_no_resize.parameters() if p.requires_grad)
    experiment_results["EfficientNetB2_NoResize"] = {
        "Accuracy": acc_no_resize, "Memory(MB)": mem_no_resize, "Inference Time(ms)": time_no_resize,
        "Params": param_count_no_resize
    }

    # --- å®éªŒ 4: EfficientNet-B2 (Pretrained) - No Label Smoothing ---
    print("\n\n=== è¿è¡Œå®éªŒ: EfficientNet-B2 (Pretrained) - No Label Smoothing ===")
    acc_no_ls, mem_no_ls, time_no_ls, model_no_ls = run_experiment(
        "EfficientNetB2_NoLS", pretrained=True, resize_factor=2, label_smoothing=0.0,  # LSè®¾ä¸º0
        random_erasing_p=0.2, base_path=results_dir
    )
    param_count_no_ls = sum(p.numel() for p in model_no_ls.parameters() if p.requires_grad)
    experiment_results["EfficientNetB2_NoLS"] = {
        "Accuracy": acc_no_ls, "Memory(MB)": mem_no_ls, "Inference Time(ms)": time_no_ls, "Params": param_count_no_ls
    }

    # --- å®éªŒ 5: EfficientNet-B2 (Pretrained) - No Random Erasing ---
    print("\n\n=== è¿è¡Œå®éªŒ: EfficientNet-B2 (Pretrained) - No Random Erasing ===")
    acc_no_re, mem_no_re, time_no_re, model_no_re = run_experiment(
        "EfficientNetB2_NoRE", pretrained=True, resize_factor=2, label_smoothing=0.1,
        random_erasing_p=0.0, base_path=results_dir  # REçš„pè®¾ä¸º0
    )
    param_count_no_re = sum(p.numel() for p in model_no_re.parameters() if p.requires_grad)
    experiment_results["EfficientNetB2_NoRE"] = {
        "Accuracy": acc_no_re, "Memory(MB)": mem_no_re, "Inference Time(ms)": time_no_re, "Params": param_count_no_re
    }

    print("\n\n========= æ‰€æœ‰å®éªŒç»“æœæ±‡æ€» =========")
    for exp_name, results in experiment_results.items():
        print(f"--- {exp_name} ---")
        for k, v in results.items():
            if k == "Params":
                print(f"  {k}: {v:,}")
            elif k == "Accuracy":
                print(f"  {k}: {v:.2f}%")
            elif k == "Memory(MB)":
                print(f"  {k}: {v:.2f} MB")
            elif k == "Inference Time(ms)":
                print(f"  {k}: {v:.2f} ms")
            else:
                print(f"  {k}: {v}")

    # å¯ä»¥å°†è¿™äº›ç»“æœä¿å­˜åˆ° CSV æˆ– JSON æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿æŠ¥å‘Šç”Ÿæˆ
    import pandas as pd

    df_results = pd.DataFrame.from_dict(experiment_results, orient='index')
    df_results.index.name = 'Experiment'
    df_results_path = os.path.join(results_dir, "ablation_study_results.csv")
    df_results.to_csv(df_results_path)
    print(f"\næ‰€æœ‰å®éªŒç»“æœå·²ä¿å­˜è‡³ {df_results_path}")

