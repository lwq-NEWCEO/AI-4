import os
import warnings
import random
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import copy

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, random_split, Subset
from torchvision import datasets, transforms, models

# å¯¼å…¥æ–°ç‰ˆæƒé‡æšä¸¾ï¼ˆå¦‚æœä¸å…¼å®¹æ—§ç‰ˆtorchvisionï¼Œä»£ç ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
try:
    from torchvision.models import efficientnet_b2, EfficientNet_B2_Weights
except ImportError:
    from torchvision.models import efficientnet_b2

    EfficientNet_B2_Weights = None

# --- é…ç½®ç¯å¢ƒ ---
warnings.filterwarnings('ignore')
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# --- 1. æ ¸å¿ƒå·¥å…·å‡½æ•° ---
def set_seed(seed=42):
    """å›ºå®šå…¨é“¾è·¯éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=15, min_delta=1e-4, path='best_model.pth'):
        self.patience = patience
        self.min_delta = min_delta
        self.path = path
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            torch.save(model.state_dict(), self.path)
        else:
            self.counter += 1
            if self.counter % 5 == 0:
                print(f"âš ï¸  æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
                print(f"ğŸš« è§¦å‘æ—©åœï¼æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")


# --- 2. æ¨¡å‹å®šä¹‰ (EfficientNet-B2) ---

class EfficientNetTransfer(nn.Module):
    def __init__(self, num_classes=10):
        super(EfficientNetTransfer, self).__init__()

        print("ğŸ”„ æ­£åœ¨åŠ è½½ EfficientNet-B2 é¢„è®­ç»ƒæƒé‡...")
        # å…¼å®¹æ–°æ—§ç‰ˆæœ¬ torchvision
        if EfficientNet_B2_Weights is not None:
            weights = EfficientNet_B2_Weights.DEFAULT
            self.base_model = efficientnet_b2(weights=weights)
        else:
            self.base_model = efficientnet_b2(pretrained=True)

        # EfficientNet çš„åˆ†ç±»å¤´é€šå¸¸å« 'classifier'
        # ç»“æ„é€šå¸¸æ˜¯: Dropout -> Linear
        # æˆ‘ä»¬éœ€è¦è·å–åŸæœ¬ Linear çš„è¾“å…¥ç‰¹å¾æ•°
        # B2 çš„ classifier[1] æ˜¯ Linear å±‚
        original_fc = self.base_model.classifier[1]
        in_features = original_fc.in_features

        # é‡æ„åˆ†ç±»å¤´
        # EfficientNet å®˜æ–¹é€šå¸¸ä½¿ç”¨ Dropout=0.3
        self.base_model.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(in_features, num_classes)
        )

    def forward(self, x):
        return self.base_model(x)

    def freeze_layers(self, freeze=True):
        """å†»ç»“/è§£å†» ç‰¹å¾æå–å±‚"""
        # å†»ç»“æ‰€æœ‰å±‚
        for param in self.base_model.parameters():
            param.requires_grad = not freeze

        # å§‹ç»ˆè§£å†»åˆ†ç±»å¤´ (classifier éƒ¨åˆ†)
        for param in self.base_model.classifier.parameters():
            param.requires_grad = True


# --- 3. è®­ç»ƒæµç¨‹å‡½æ•° ---

def train_stage_model(model, train_loader, val_loader, epochs, optimizer, scheduler, early_stopping, stage_name):
    # ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ï¼Œè¿™åœ¨ EfficientNet è®ºæ–‡ä¸­è¢«å¼ºçƒˆæ¨è
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}
    best_val_acc = 0.0

    print(f"\n===== å¼€å§‹ {stage_name}ï¼ˆå…± {epochs} Epochsï¼‰=====")

    for epoch in range(epochs):
        # Train
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0

        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs} [Train]", leave=False)
        for inputs, labels in train_bar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        # Val
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        # Metrics
        avg_train_loss = train_loss / train_total
        avg_train_acc = 100 * train_correct / train_total
        avg_val_loss = val_loss / val_total
        avg_val_acc = 100 * val_correct / val_total
        current_lr = optimizer.param_groups[0]['lr']

        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc

        print(f"Ep {epoch + 1}/{epochs} | LR: {current_lr:.2e} | "
              f"Tr Loss: {avg_train_loss:.4f} Acc: {avg_train_acc:.2f}% | "
              f"Val Loss: {avg_val_loss:.4f} Acc: {avg_val_acc:.2f}% (Best: {best_val_acc:.2f}%)")

        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)
        history['lr'].append(current_lr)

        early_stopping(avg_val_loss, model)
        if early_stopping.early_stop:
            break

        scheduler.step()

    return history


def evaluate_test_set(model, test_loader, classes, device):
    model.eval()
    test_correct = 0
    test_total = 0
    class_correct = list(0. for _ in range(len(classes)))
    class_total = list(0. for _ in range(len(classes)))

    print("\nğŸ” æ­£åœ¨è¯„ä¼°æµ‹è¯•é›†...")
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Testing"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()
            c = (predicted == labels).squeeze()
            for i in range(len(labels)):
                label = labels[i]
                class_correct[label] += c[i].item()
                class_total[label] += 1

    acc = 100 * test_correct / test_total
    print(f"\nğŸ“Š æµ‹è¯•é›†æœ€ç»ˆå‡†ç¡®ç‡: {acc:.2f}%")
    print("-" * 30)
    for i in range(len(classes)):
        if class_total[i] > 0:
            print(f"  {classes[i]:<10}: {100 * class_correct[i] / class_total[i]:.2f}%")
    print("-" * 30)
    return acc


def plot_history(hist1, hist2, filename='effnet_b2_history.png'):
    loss = hist1['train_loss'] + hist2['train_loss']
    val_loss = hist1['val_loss'] + hist2['val_loss']
    acc = hist1['train_acc'] + hist2['train_acc']
    val_acc = hist1['val_acc'] + hist2['val_acc']
    lr = hist1['lr'] + hist2['lr']

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(loss, label='Train Loss')
    plt.plot(val_loss, label='Val Loss')
    plt.axvline(x=len(hist1['train_loss']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 2)
    plt.plot(acc, label='Train Acc')
    plt.plot(val_acc, label='Val Acc')
    plt.axvline(x=len(hist1['train_acc']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 3)
    plt.plot(lr, label='LR', color='green')
    plt.axvline(x=len(hist1['lr']) - 1, color='r', linestyle='--', alpha=0.5, label='Stage 1 End')
    plt.title('Learning Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(filename, dpi=300)
    print(f"\nğŸ“ˆ æ›²çº¿å›¾å·²ä¿å­˜è‡³ {filename}")


# --- ä¸»ç¨‹åº ---
if __name__ == '__main__':
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Device: {device}")

    # 1. æ•°æ®å‡†å¤‡
    # [å…³é”®ä¼˜åŒ–] EfficientNet éœ€è¦ç¨å¤§çš„åˆ†è¾¨ç‡æ‰èƒ½å‘æŒ¥æ€§èƒ½
    # å°† 32x32 Resize åˆ° 64x64ï¼Œèƒ½æ˜¾è‘—æå‡å‡†ç¡®ç‡
    RESIZE_SIZE = 64

    transform_train = transforms.Compose([
        transforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),  # æ”¾å¤§å›¾ç‰‡
        transforms.RandomCrop(RESIZE_SIZE, padding=8),  # é€‚åº”æ›´å¤§çš„å›¾
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.2)
    ])

    transform_test = transforms.Compose([
        transforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),  # æµ‹è¯•é›†ä¹Ÿè¦æ”¾å¤§
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    full_data = datasets.CIFAR10(root='./data', train=True, download=False)
    train_idx, val_idx = random_split(range(len(full_data)), [45000, 5000])

    train_ds = Subset(datasets.CIFAR10('./data', train=True, transform=transform_train), train_idx.indices)
    val_ds = Subset(datasets.CIFAR10('./data', train=True, transform=transform_test), val_idx.indices)
    test_ds = datasets.CIFAR10('./data', train=False, transform=transform_test)

    # EfficientNet B2 æ˜¾å­˜å ç”¨è¾ƒå¤§ï¼Œå»ºè®® Batch Size è®¾ç½®ä¸º 64 æˆ– 128
    # å¦‚æœ 128 çˆ†æ˜¾å­˜ï¼Œè¯·æ”¹ä¸º 64
    BATCH_SIZE = 128
    workers = 4 if device.type == 'cuda' else 0

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=workers, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=True)

    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # 2. åˆå§‹åŒ– EfficientNet-B2
    print("\nğŸ”¨ åˆå§‹åŒ– EfficientNet-B2 ...")
    model = EfficientNetTransfer(num_classes=10).to(device)

    # 3. é˜¶æ®µ 1: å†»ç»“è®­ç»ƒ (Warmup)
    print("\n>>> é˜¶æ®µ 1: å†»ç»“ç‰¹å¾å±‚ (Warmup Classifier)")
    model.freeze_layers(freeze=True)
    # AdamW éå¸¸é€‚åˆ EfficientNet
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)
    stopper = EarlyStopping(patience=5, path='effnet_b2_best.pth')

    hist1 = train_stage_model(model, train_loader, val_loader, 5, optimizer, scheduler, stopper, "Stage 1")

    # 4. é˜¶æ®µ 2: å…¨é¢å¾®è°ƒ
    print("\n>>> é˜¶æ®µ 2: è§£å†»å…¨ç½‘å¾®è°ƒ (Fine-tuning)")
    model.load_state_dict(torch.load('effnet_b2_best.pth'))
    model.freeze_layers(freeze=False)

    # EfficientNet å¾®è°ƒå­¦ä¹ ç‡ï¼š1e-4 æ˜¯ä¸ªé»„é‡‘å€¼
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)
    # 150 ä¸ª Epochs è¶³å¤Ÿå®ƒæ”¶æ•›
    scheduler = CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-7)
    stopper = EarlyStopping(patience=20, path='effnet_b2_best.pth')

    hist2 = train_stage_model(model, train_loader, val_loader, 150, optimizer, scheduler, stopper, "Stage 2")

    # 5. ç»“æœ
    plot_history(hist1, hist2)

    print("\nğŸ† åŠ è½½æœ€ç»ˆæœ€ä½³æ¨¡å‹...")
    model.load_state_dict(torch.load('effnet_b2_best.pth'))
    evaluate_test_set(model, test_loader, classes, device)

    torch.save(model.state_dict(), 'final_effnet_b2_95acc.pth')
    print("\nâœ… å®Œæˆï¼EfficientNet-B2 æ¨¡å‹å·²ä¿å­˜ã€‚")
